\documentclass[11pt]{article}
\usepackage{soul}
\usepackage{amsmath}
\usepackage{float}
% \pagestyle{empty}

\setlength{\oddsidemargin}{-0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.9 in}
\setlength{\textwidth}{7.0 in}
\setlength{\textheight}{9.0 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0.3 in}
\setlength{\parskip}{0.1 in}
\usepackage{epsf}

\def\O{\mathop{\smash{O}}\nolimits}
\def\o{\mathop{\smash{o}}\nolimits}
\newcommand{\e}{{\rm e}}
\newcommand{\R}{{\bf R}}
\newcommand{\Z}{{\bf Z}}

\usepackage{xcolor}

\newcommand{\mnote}[1]{{\color{red} [Madhu: #1]}}
\newcommand{\achnote}[1]{{\color{orange} [Adam: #1]}}

\begin{document}
	\section*{CS 1240 Programming Assignment 1: Spring 2026}
	\textbf{Your name(s) (up to two):} Annika Palm and Michelle Weon \\
	\noindent \textbf{Collaborators:} None \\
	\noindent \textbf{No. of late days used on previous psets: } (3 for annika)\\
	\noindent \textbf{No. of late days used after including this pset: } (6 for annika) \\
    
\noindent {\bf {\Large Overview}:}
The purpose of this assignment is to experience some of the problems
involved with implementing an algorithm (in this case, a minimum
spanning tree algorithm) in practice.  As an added benefit, we will
explore how minimum spanning trees behave in random graphs.


\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \textbf{n}& \textbf{Average tree size}\\
        ------ & --------------------------- \\
        128 & 1.2907931698693316\\
        256 &1.0985369488383596 \\
        512 & 1.1949541667887964 \\
        1024 & 1.1791199032776452 \\
        2048 &  1.1984589896052946\\
        4096 &1.1909194425320209  \\
        8192 & 1.1981356554288438 \\
        16384 & 1.1969435590125737\\
        32768 &  1.1981037806374797\\
    \end{tabular}
    \caption{Complete Graph (dimension 1)}
    \label{tab:placeholder}
\end{table}

We guess that our function is $f(n) = 1.2$. No matter the value of $n$, our average tree size remains as  $\approx 1.2$
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \textbf{n} & \textbf{Average tree size}\\
        ------ & --------------------------- \\
        128 & 19.500709799287574 \\
        256 &35.25277856873798 \\
        512 & 64.75717734931135\\
        1024 & 115.9997225025061 \\
        2048 &  209.5600564819763\\
        4096 & 388.990443203373  \\
        8192 & 721.3228271932845\\
        16384 & 1351.2676259369564\\
        32768 &  2522.802715814287\\
        65536 & 4740.6146319871805 \\
        131072 &  8924.969706574975\\
        262144 & 16869.504554608346  \\
    \end{tabular}
    \caption{Hypercube graph}
    \label{tab:placeholder}
\end{table}

We guess that our function is $f(n) = 0.65n$. The function appears linear with $f = \Theta(n)$, and we can calculate the rate of change between each value to find that the total average rate of change is $\approx 0.65$
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \textbf{n} & \textbf{Average tree size}\\
        ------ & --------------------------- \\
        128 & 7.787736965766063\\
        256 &10.484601468647364 \\
        512 & 14.905898534580235 \\
        1024 & 21.253455806066036 \\
        2048 &  29.654844756531027\\
        4096 & 41.68774820407019 \\
        8192 &  58.87139177359353\\
        16384 &  83.2404638477565\\
        32768 &  117.4690205742614\\
    \end{tabular}
    \caption{Complete graph (dimension 2)}
    \label{tab:placeholder}
\end{table}

We guess that our function is $f(n) = 0.65 \sqrt{ n}$. When graphing the values, we see that it closely resembles the graph of $\sqrt{n}$, and we found the constant factor to be $\approx 0.65$
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \textbf{n} & \textbf{Average tree size}\\
        ------ & --------------------------- \\
        128 &  17.72625348332577 \\
        256 &27.741390212388104 \\
        512 & 43.39131603531435\\
        1024 & 67.62254279850062\\
        2048 &  106.90284877829802\\
        4096 & 168.90750651946624  \\
        8192 &  267.20391878535025\\
        16384 &  422.398589889808\\
        32768 &  667.7341810821756\\
    \end{tabular}
    \caption{Complete graph (dimension 3)}
    \label{tab:placeholder}
\end{table}
We guess that our function is $f(n) = 0.65n^{2/3}$. Our graph most closely resembled the graph of $n^{2/3}$ and found our constant factor to once again be $\approx 0.65$
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \textbf{n} & \textbf{Average tree size}\\
        ------ & --------------------------- \\
        128 &  28.15867397181637\\
        256 & 47.91248314776185 \\
        512 & 78.64492120726746 \\
        1024 & 130.80037531694546 \\
        2048 &  216.59692894777567\\
        4096 &  359.3070913867543\\
        8192 & 604.3192343549927  \\
        16384 & 1008.1351156849811  \\
        32768 &  1689.7959194709474\\
    \end{tabular}
    \caption{Complete graph (dimension 4)}
    \label{tab:placeholder}
\end{table}

We guess that our function is $f(n) = 0.7n^{3/4}$. Our closely resembled the graph of $n^{3/4}$ and we found our constant factor this time to be $\approx 0.7$
\newline
\newline
\textbf{\large{Discussion:}}
\newline
Our implementation uses different MST strategies depending on the graph dimension in order to balance correctness and efficiency. In dimension $0$ (the complete graph with independent $\text{Uniform}[0,1]$ edge weights), explicitly constructing all $\Theta(n^2)$ edges would be computationally infeasible for large $n$. Instead, we use a sparsification technique inspired by the hint in the assignment. For each possible edge $(i,j)$, we generate its weight on the fly and only keep the edge if its weight is at most $k(n) = \frac{15 \log(n+1)}{n+1}$

We then sort the remaining edges and run Kruskal’s algorithm using a Union-Find data structure with path compression and union by rank. Since the MST of this random graph is extremely unlikely to include edges heavier than $k(n)$, this pruning preserves correctness with high probability while dramatically reducing the number of edges we need to process. Without this filtering step, sorting $\Theta(n^2)$ edges would have made large inputs impractical.

For dimension $1$ (the hypercube graph), we explicitly construct the adjacency list. Each vertex connects to other vertices obtained by flipping a single bit in its index, so each vertex has approximately $\log n$ neighbors. This yields a total of $\Theta(n \log n)$ edges. We then compute the MST using Kruskal’s algorithm on this edge set. Because the number of edges grows only as $n \log n$, sorting remains manageable, and this is why we were able to scale up to $n = 262{,}144$ in this case.

For dimensions $2$, $3$, and $4$, we generate $n$ random points in the unit square, cube, or hypercube, respectively, and compute Euclidean distances between points. Constructing all $\Theta(n^2)$ edges would again be too slow, so we use a grid-based spatial partitioning method. The theoretical maximum edge length in the MST is on the order of $(\frac{\log n}{n})^{1/d}$ so we divide the space into grid cells of comparable size and only consider edges between points in the same cell or neighboring cells. This ensures that each vertex connects only to nearby points, reducing the expected number of edges to roughly $O(n)$ instead of $O(n^2)$. After building this sparse adjacency structure, we run an eager version of Prim’s algorithm using an indexed min-heap that supports decrease-key operations. This keeps the runtime approximately $O(n \log n)$ in expectation.

In terms of asymptotic runtime, the dimension $0$ case runs in roughly $O(n \log^2 n)$ time after sparsification, since we expect about $O(n \log n)$ edges to survive the weight threshold and sorting dominates. The hypercube case also runs in approximately $O(n \log^2 n)$ time due to sorting $\Theta(n \log n)$ edges. For dimensions $2$–$4$, the grid-based pruning reduces the expected number of edges to linear in $n$, so Prim’s algorithm runs in roughly $O(n \log n)$ time. In practice, higher dimensions incur larger constant factors because the number of neighboring grid cells grows as $3^d$, which explains why the $4$D case was noticeably slower than $2$D.

The observed growth rates of the MST weight match known theoretical results. In the complete graph with independent random edge weights, the expected total MST weight converges to a constant as $n \to \infty$. Intuitively, as $n$ increases, extremely small edge weights become available, and the MST primarily uses those edges, so the total weight does not grow with $n$.

For the hypercube graph, each vertex has only $\Theta(\log n)$ neighbors, so the number of candidate edges does not grow quadratically. As a result, the minimum available edge weights do not shrink as quickly as in the complete graph, and the expected weight per MST edge remains roughly constant. Since the MST has $n-1$ edges, the total weight grows linearly with $n$.

For the geometric graphs in $d$ dimensions, points are distributed uniformly in the unit hypercube. The typical spacing between nearest neighbors is approximately $n^{-1/d}$, and the MST contains $n-1$ edges, so the total weight scales as  $n \cdot n^{-1/d} = n^{(d-1)/d}$

Our experimental results closely match this prediction: the $2$D case behaves like $n^{1/2}$, the $3$D case like $n^{2/3}$, and the $4$D case like $n^{3/4}$.

In terms of practical performance, the densest case would have been dimension $0$ without sparsification, but the filtering step made it feasible for large $n$. The hypercube scaled particularly well because it is inherently sparse. The geometric cases became slower in higher dimensions due to increased constant factors from checking more neighboring cells and computing higher-dimensional distances. We also observed slowdowns at very large $n$, likely due to memory usage and cache effects.

Finally, we used Python’s built-in \texttt{random.random()} as our pseudorandom number generator. Across multiple trials, the averages converged smoothly and showed no irregular behavior. Because the results were stable and consistent across independent runs, we believe the random number generator was sufficiently reliable for this assignment.


\end{document}
